#!/usr/bin/env python3
"""
Test the complete FREE AI pipeline
Uses Ollama + sentence-transformers (100% FREE!)
"""

import sys
sys.path.append('.')

from app.config.loader import load_org_config
from app.rag.vector_store import VectorStore
from app.rag.local_embeddings import LocalEmbedder
from app.llm.local_responder import generate_response_local, test_local_llm

print("=" * 60)
print("ðŸ†“ FREE AI Voice Agent Pipeline Test")
print("=" * 60)

# Test 1: Check Ollama
print("\n1. Testing Ollama LLM...")
working, models = test_local_llm()
if working:
    print(f"âœ“ Ollama is running!")
    print(f"âœ“ Available models: {', '.join(models)}")
else:
    print("âœ— Ollama not running")
    print("Start with: ollama serve")
    print("Then: ollama pull llama3.2:3b")

# Test 2: Config Loading
print("\n2. Testing Config Loader...")
config = load_org_config("sample_org")
print(f"âœ“ Loaded config for: {config['org_name']}")
print(f"âœ“ Assistant role: {config['assistant_role']}")

# Test 3: Load Vector Store
print("\n3. Testing Vector Store (FREE embeddings)...")
try:
    vector_store = VectorStore(dimension=384)  # Local embeddings are 384-dim
    vector_store.load("data/sample_org/vector_store_local")
    print(f"âœ“ Loaded vector store with {len(vector_store.chunks)} documents")
except Exception as e:
    print(f"âœ— Vector store not found: {e}")
    print("Run: python scripts/ingest_documents_free.py --org sample_org")
    sys.exit(1)

# Test 4: Semantic Search
print("\n4. Testing Semantic Search...")
try:
    embedder = LocalEmbedder("all-MiniLM-L6-v2")
    
    query = "What are your operating hours?"
    print(f"Query: '{query}'")
    
    # Create query embedding
    query_embedding = embedder.embed_text(query)
    print(f"âœ“ Created query embedding (dim: {len(query_embedding)})")
    
    # Search vector store
    results = vector_store.search(query_embedding, k=2)
    print(f"âœ“ Found {len(results)} relevant documents")
    
    for i, (chunk, meta, distance) in enumerate(results, 1):
        print(f"\n  Result {i} (distance: {distance:.2f}):")
        print(f"  {chunk[:150]}...")
    
    # Extract context chunks
    context_chunks = [chunk for chunk, _, _ in results]
    
except Exception as e:
    print(f"âœ— Search failed: {e}")
    context_chunks = []

# Test 5: LLM Response
print("\n5. Testing LLM Response Generation...")
if working and context_chunks:
    response = generate_response_local(
        user_text=query,
        context_chunks=context_chunks,
        model="llama3.2:3b"
    )
    print(f"âœ“ Generated response:")
    print(f"\n  {response}\n")
else:
    print("âœ— Skipping (Ollama not available or no context)")

# Summary
print("=" * 60)
print("âœ… FREE Pipeline Test Complete!")
print("=" * 60)

print("\nðŸ’° Cost Breakdown:")
print("  Ollama (LLM): $0.00 (runs locally)")
print("  sentence-transformers (Embeddings): $0.00 (runs locally)")
print("  Vector Store (FAISS): $0.00 (runs locally)")
print("  Total per call: $0.00 âœ¨")

print("\nâš¡ Performance:")
print("  - LLM: Faster than GPT-4 (local)")
print("  - Embeddings: Slightly lower accuracy (384-dim vs 1536-dim)")
print("  - Overall: Good enough for development and learning!")

print("\nðŸŽ“ What You Learned:")
print("  âœ“ Document processing and chunking")
print("  âœ“ Vector embeddings and similarity search")
print("  âœ“ RAG (Retrieval Augmented Generation)")
print("  âœ“ LLM integration")
print("  âœ“ Complete AI pipeline architecture")

print("\nðŸ’¡ Next Steps:")
print("  1. Test with different questions")
print("  2. Add your own documents")
print("  3. Customize the prompts")
print("  4. When ready, upgrade to OpenAI for production")

print("\nðŸš€ Your Voice Agent is 100% functional (FREE)!")
